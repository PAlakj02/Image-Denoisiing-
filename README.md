# Image Denoising using GANs

## Introduction

Image denoising is a critical task in computer vision, aiming to recover high-quality images from noisy inputs. This project employs a Generative Adversarial Network (GAN) based architecture to denoise bw images and convert them into high-resolution color images. The architecture is inspired by the seminal paper "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks" by Radford et al. (2015). Despite the promising framework, the final Peak Signal-to-Noise Ratio (PSNR) of 4.85 was significantly lower than expected.

## Architecture

### Generator

The Generator's primary role is to produce a denoised and high-resolution color image from a bw input. It employs a deep convolutional neural network (CNN) inspired by Radford et al. The key layers and their parameters are as follows:

- **Input Layer:** Accepts a bw image.
- **Convolutional Layers:** Multiple layers with increasing filter sizes, each followed by batch normalization and ReLU activation to extract complex features from the input image.
- **Upsampling Layers:** Utilized to increase the resolution of the image progressively. This is achieved using transposed convolutions (also known as deconvolutions) to scale the image to a higher resolution.
- **Output Layer:** A convolutional layer with a Tanh activation function to output the final denoised and colorized image.

### Discriminator

The Discriminator's task is to distinguish between real high-resolution color images and those generated by the Generator. It also employs a deep CNN with the following layers:

- **Input Layer:** Accepts an image (either real or generated).
- **Convolutional Layers:** Multiple layers with decreasing filter sizes, each followed by batch normalization and Leaky ReLU activation to capture discriminative features.
- **Output Layer:** A single neuron with a Sigmoid activation function to output a probability indicating whether the input image is real or generated.

## Training Process

### Dataset

The training dataset consists of bw and high-resolution color images. The bw images is used as an input . The high-resolution color images serve as the ground truth for the Generator.

### Training Procedure

The GAN is trained in a two-step process:

1. **Train the Discriminator:**
   - **Real Images:** The Discriminator is trained with real high-resolution color images, labeled as real.
   - **Generated Images:** The Generator creates images from bw inputs, which are then labeled as fake for the Discriminator.
   - **Loss Function:** Binary cross-entropy loss is used to measure the Discriminator's performance.

2. **Train the Generator:**
   - The Generator aims to produce images that the Discriminator cannot distinguish from real images.
   - The Generator's loss is a combination of the adversarial loss (binary cross-entropy) and the L2 loss (mean squared error) between the generated image and the ground truth.
   - The training alternates between updating the Discriminator and the Generator to ensure a balanced training process.
Testing
Prepare the Test Dataset:

Place your noisy grayscale test images in the test/low/ directory.
Place the corresponding clean high-resolution images in the test/predicted/ directory.
Run the Testing Script:

The testing process is managed by the main.py file. Ensure main.py is configured to load the test dataset.

Execute the testing script:

``bash 
  python main.py
The script will generate denoised images and save them in the specified output directory. It will also calculate and display the PSNR for each test image.
### Evaluation Metrics

The primary metric used to evaluate the performance of the denoised images is the Peak Signal-to-Noise Ratio (PSNR). PSNR measures the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation.

## Implementation Details

### Software and Libraries

- **Python:** Version 3.11
- **PyTorch:** Used for building and training the GAN model.
- **NumPy:** Utilized for numerical operations and data manipulation.
- **OpenCV:** Used for image processing tasks, such as reading, writing, and manipulating images.
- **Matplotlib:** Employed for visualizing the training process and results.

### Training Environment

- **Hardware:** The model was trained on cpu
- **Software Environment:** The project was developed in a virtual environment to manage dependencies effectively and ensure compatibility.

## Results

The training process resulted in a PSNR of 4.85, which is considerably lower than anticipated. This suggests that the generated images are still significantly noisy and of lower quality than expected.

### Analysis of Results

Several factors might have contributed to the suboptimal PSNR:
- **Network Architecture:** Despite being inspired by Radford et al., the chosen architecture may not have been well-suited for this specific task.
- **Training Data:** The quality of training data might have been insufficient to train the GAN effectively.
- **Hyperparameters:** Suboptimal choices of learning rates, batch sizes, and other hyperparameters could have hindered the training process.

## Conclusion

This project explored the application of GANs for denoising bw images and converting them into high-resolution color images. While the framework showed potential, the resulting PSNR of 4.85 indicates significant room for improvement. Due to constraints, further modifications to the Generator or Discriminator architectures were not feasible. Future work could focus on optimizing the network architectures, experimenting with different loss functions, and augmenting the training dataset to enhance performance.

## References

Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arXiv preprint arXiv:1511.06434.

## How to Use

1. **Clone the Repository:**
   ```bash
   git clone <repository-url>
   cd <repository-directory>
